{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import wandb\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Input, Conv2D, Dense, Flatten, Dropout, GlobalMaxPooling2D, MaxPooling2D, BatchNormalization\n",
    "from tensorflow.keras.models import Model\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "\n",
    "# Set this flag for different datasets\n",
    "#dataset = 'mnist'\n",
    "dataset = 'cifar'"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if dataset == 'mnist':\n",
    "    fashion_mnist = tf.keras.datasets.fashion_mnist\n",
    "    (X_train, y_train), (X_test, y_test) = fashion_mnist.load_data()\n",
    "    X_train, X_test = X_train/255.0, X_test/255.0\n",
    "    X_train, X_test = np.expand_dims(X_train, -1), np.expand_dims(X_test, -1)\n",
    "elif dataset == 'cifar':\n",
    "    cifar10 = tf.keras.datasets.cifar10\n",
    "    (X_train, y_train), (X_test, y_test) = cifar10.load_data()\n",
    "    X_train, X_test = X_train/255.0, X_test/255.0\n",
    "    y_train, y_test = y_train.flatten(), y_test.flatten()\n",
    "else:\n",
    "    raise ValueError('Dataset could not be determined')\n",
    "\n",
    "# Number of classes\n",
    "K = len(set(y_train))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure the wandb sweep\n",
    "sweep_config = {\n",
    "    'method': 'bayes',\n",
    "    'metric': {\n",
    "      'name': 'val_loss',\n",
    "      'goal': 'minimize'   \n",
    "    },\n",
    "    'parameters': {\n",
    "        'learning_rate': {\n",
    "            'values': [1, 0.1, 0.01, 0.001, 0.0001]\n",
    "        },\n",
    "        'epochs': {\n",
    "            'values': [5, 10, 20]\n",
    "        },\n",
    "        'batch_size': {\n",
    "            'values': [16, 32, 64, 128, 256]\n",
    "        },\n",
    "        'optimizer': {\n",
    "            'values': ['adam', 'nadam', 'sgd', 'rmsprop']\n",
    "        },\n",
    "        'conv_layers': {\n",
    "            'values': [4, 6, 8]\n",
    "        },\n",
    "        'kernel_size': {\n",
    "            'values': [3, 5, 7]\n",
    "        },\n",
    "        'dense_layers': {\n",
    "            'values': [1, 2, 3]\n",
    "        },\n",
    "        'dropout': {\n",
    "            'values': [0.3, 0.2, 0.1]\n",
    "        },\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialise sweep\n",
    "sweep_id = wandb.sweep(sweep_config, project='tensorflow-test', entity='kavp')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mega function to define and train model and log results (used by the sweep)\n",
    "def sweep_func():\n",
    "    # Default hyperparameter values\n",
    "    config_defaults = {\n",
    "        'learning_rate': 0.001,\n",
    "        'epochs': 50,\n",
    "        'batch_size': 128,\n",
    "        'optimizer': 'adam',\n",
    "        'conv_layers': 6,\n",
    "        'kernel_size': 3,\n",
    "        'dense_layers': 2,\n",
    "        'dropout': 0.2,\n",
    "        'eager_mode': False,\n",
    "    }\n",
    "\n",
    "    # Initialise run\n",
    "    wandb.init(config=config_defaults)\n",
    "\n",
    "    # Variable to hold the sweep values\n",
    "    config = wandb.config\n",
    "    \n",
    "    if config['eager_mode'] == True:\n",
    "        tf.compat.v1.enable_eager_execution()\n",
    "    elif config['eager_mode'] == False:\n",
    "        tf.compat.v1.disable_eager_execution()\n",
    "    else:\n",
    "        raise ValueError('eager_mode property of wandb config could not be determined.') \n",
    "\n",
    "    num_layers = int(config['conv_layers']/2)\n",
    "    # Build model with keras functional API\n",
    "    inp = Input(shape=X_train[0].shape)\n",
    "    x = Conv2D(32, config['kernel_size'], activation='relu', padding='same')(inp)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Conv2D(32, config['kernel_size'], activation='relu', padding='same')(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = MaxPooling2D((2,2))(x)\n",
    "    for i in range(1,num_layers):\n",
    "        x = Conv2D(32*pow(2, i), config['kernel_size'], activation='relu', padding='same')(x)\n",
    "        x = BatchNormalization()(x)\n",
    "        x = Conv2D(32*pow(2, i), config['kernel_size'], activation='relu', padding='same')(x)\n",
    "        x = BatchNormalization()(x)\n",
    "        x = MaxPooling2D((2,2))(x)\n",
    "    x = Flatten()(x)\n",
    "    for i in range(config['dense_layers']-1):\n",
    "        x = Dropout(config['dropout'])(x)\n",
    "        x = Dense(1024/(i+1), activation='relu')(x)\n",
    "    x = Dropout(config['dropout'])(x)\n",
    "    x = Dense(K, activation='softmax')(x)\n",
    "\n",
    "    model = Model(inp, x)\n",
    "\n",
    "    model.compile(\n",
    "        optimizer=config['optimizer'],\n",
    "        loss='sparse_categorical_crossentropy',\n",
    "        metrics=['accuracy'],\n",
    "        run_eagerly=config['eager_mode'],\n",
    "    )\n",
    "\n",
    "    # Data augmentation\n",
    "    data_generator = tf.keras.preprocessing.image.ImageDataGenerator(width_shift_range=0.1, height_shift_range=0.1, horizontal_flip=True)\n",
    "    train_generator = data_generator.flow(X_train, y_train, config['batch_size'])\n",
    "    steps_per_epoch = X_train.shape[0] // config['batch_size']\n",
    "\n",
    "    with tf.compat.v1.Session() as sess:\n",
    "        r = model.fit_generator(train_generator, validation_data=(X_test,y_test), epochs=config['epochs'], steps_per_epoch=steps_per_epoch)\n",
    "        wandb.tensorflow.log(tf.compat.v1.summary.merge_all())\n",
    "        wandb.log({'loss': r.history['loss'][-1], 'val_loss': r.history['val_loss'][-1], 'accuracy': r.history['accuracy'][-1], 'val_accuracy': r.history['val_accuracy'][-1]})\n",
    "\n",
    "        wandb_data = [[x,y] for (x,y) in zip(np.arange(0, config['epochs'], 1), r.history['loss'])]\n",
    "        table = wandb.Table(data=wandb_data, columns = [\"epoch\", \"loss\"])\n",
    "        wandb.log({\"loss_against_epochs\" : wandb.plot.line(table, \"epoch\", \"loss\", title=\"Training loss\")})\n",
    "\n",
    "        wandb_data = [[x,y] for (x,y) in zip(np.arange(0, config['epochs'], 1), r.history['val_loss'])]\n",
    "        table = wandb.Table(data=wandb_data, columns = [\"epoch\", \"val_loss\"])\n",
    "        wandb.log({\"val_loss_against_epochs\" : wandb.plot.line(table, \"epoch\", \"val_loss\", title=\"Validation loss\")})\n",
    "\n",
    "        wandb_data = [[x,y] for (x,y) in zip(np.arange(0, config['epochs'], 1), r.history['accuracy'])]\n",
    "        table = wandb.Table(data=wandb_data, columns = [\"epoch\", \"accuracy\"])\n",
    "        wandb.log({\"accuracy_against_epochs\" : wandb.plot.line(table, \"epoch\", \"accuracy\", title=\"Training accuracy\")})\n",
    "\n",
    "        wandb_data = [[x,y] for (x,y) in zip(np.arange(0, config['epochs'], 1), r.history['val_accuracy'])]\n",
    "        table = wandb.Table(data=wandb_data, columns = [\"epoch\", \"val_accuracy\"])\n",
    "        wandb.log({\"val_accuracy_against_epochs\" : wandb.plot.line(table, \"epoch\", \"val_accuracy\", title=\"Validation accuracy\")})\n",
    "\n",
    "        # Save model\n",
    "        model.save(os.path.join(wandb.run.dir, 'model.h5'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb.agent(sweep_id, sweep_func)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb.finish()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load a good run\n",
    "good_run_path = 'kavp/tensorflow-test/gacty1xz/'\n",
    "run = wandb.Api().run(good_run_path)\n",
    "# Load model\n",
    "best_model = wandb.restore('model.h5', run_path=good_run_path)\n",
    "# Load config\n",
    "config = run.config\n",
    "\n",
    "# New model\n",
    "num_layers = int(config['conv_layers']/2)\n",
    "# Build model with keras functional API\n",
    "inp = Input(shape=X_train[0].shape)\n",
    "x = Conv2D(32, config['kernel_size'], activation='relu', padding='same')(inp)\n",
    "x = BatchNormalization()(x)\n",
    "x = Conv2D(32, config['kernel_size'], activation='relu', padding='same')(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = MaxPooling2D((2,2))(x)\n",
    "for i in range(1,num_layers):\n",
    "    x = Conv2D(32*pow(2, i), config['kernel_size'], activation='relu', padding='same')(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Conv2D(32*pow(2, i), config['kernel_size'], activation='relu', padding='same')(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = MaxPooling2D((2,2))(x)\n",
    "x = Flatten()(x)\n",
    "for i in range(config['dense_layers']-1):\n",
    "    x = Dropout(config['dropout'])(x)\n",
    "    x = Dense(1024/(i+1), activation='relu')(x)\n",
    "x = Dropout(config['dropout'])(x)\n",
    "x = Dense(K, activation='softmax')(x)\n",
    "\n",
    "model = Model(inp, x)\n",
    "\n",
    "model.compile(\n",
    "    optimizer=config['optimizer'],\n",
    "    loss='sparse_categorical_crossentropy',\n",
    "    metrics=['accuracy'],\n",
    "    run_eagerly=config['eager_mode'],\n",
    ")\n",
    "\n",
    "# Load its weights into the new model\n",
    "model.load_weights(best_model.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtain some predictions and plot the confusion matrix\n",
    "# argmax to get predictions from one-hot encoded vectors\n",
    "predicted_class = tf.argmax(model.predict(X_test),1)\n",
    "tf.math.confusion_matrix(y_test, predicted_class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Labels\n",
    "if dataset == 'mnist':\n",
    "    labels = ['t-shirt', 'trousers', 'pullover', 'dress', 'coat', 'sandal', 'shirt', 'sneaker', 'bag', 'ankle boot']\n",
    "elif dataset == 'cifar':\n",
    "    labels = ['airplane', 'automobile', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck']\n",
    "else:\n",
    "    raise ValueError('Dataset could not be determined')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show some misclassified examples\n",
    "misclassified = np.where(predicted_class != y_test)[0]\n",
    "index = np.random.choice(misclassified)\n",
    "plt.imshow(X_test[index].squeeze(), cmap='gray')\n",
    "plt.title('Ground truth: %s    Predicted: %s' % (labels[y_test[index]], labels[tf.get_static_value(predicted_class[index])]));"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9 (tags/v3.10.9:1dd9be6, Dec  6 2022, 20:01:21) [MSC v.1934 64 bit (AMD64)]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "4ae95049be29ccc4e9e63d75792906e6275d896119b1dc25df5d9e2259f80455"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
