{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import wandb\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'https://raw.githubusercontent.com/lazyprogrammer/machine_learning_examples/master/tf2.0/moore.csv'\n",
    "filename = 'moores_law_data.data'\n",
    "filepath = tf.keras.utils.get_file(filename, url)\n",
    "target_path = '../resources/' + filename\n",
    "os.system('move ' + filepath + ' ' + target_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('../resources/moores_law_data.data', header=None).to_numpy()\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = data[:,0].reshape(-1,1)\n",
    "Y = np.log(data[:,1]) # Log transformation for linear relationship\n",
    "plt.scatter(X,Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Partially normalise\n",
    "X = X - X.mean()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure the wandb sweep\n",
    "sweep_config = {\n",
    "    'method': 'bayes',\n",
    "    'metric': {\n",
    "      'name': 'loss',\n",
    "      'goal': 'minimize'   \n",
    "    },\n",
    "    'parameters': {\n",
    "        'learning_rate': {\n",
    "            'values': [0.001, 0.005, 0.0005]\n",
    "        },\n",
    "        'epochs': {\n",
    "            'values': [100, 200, 500]\n",
    "        },\n",
    "        'batch_size': {\n",
    "            'values': [32, 64, 128]\n",
    "        },\n",
    "        'optimizer': {\n",
    "            'values': ['adam', 'nadam', 'rmsprop', 'adadelta', 'adagrad', 'adamax']\n",
    "        },\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialise sweep\n",
    "sweep_id = wandb.sweep(sweep_config, project='tensorflow-test', entity='kavp')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mega function to define and train model and log results (used by the sweep)\n",
    "def sweep_func():\n",
    "    # Default hyperparameter values\n",
    "    config_defaults = {\n",
    "        'learning_rate': 0.001,\n",
    "        'epochs': 100,\n",
    "        'batch_size': 128,\n",
    "        'optimizer': 'adam',\n",
    "        'eager_mode': False,\n",
    "        'loss_func': 'mse',\n",
    "        'callbacks': 'lr',\n",
    "        'momentum': 0.9,\n",
    "    }\n",
    "\n",
    "    # Initialise run\n",
    "    wandb.init(config=config_defaults)\n",
    "\n",
    "    # Variable to hold the sweep values\n",
    "    config = wandb.config\n",
    "    \n",
    "    if config['eager_mode'] == True:\n",
    "        tf.compat.v1.enable_eager_execution()\n",
    "    elif config['eager_mode'] == False:\n",
    "        tf.compat.v1.disable_eager_execution()\n",
    "    else:\n",
    "        raise ValueError('eager_mode property of wandb config could not be determined.') \n",
    "\n",
    "    # Define model\n",
    "    model = tf.keras.models.Sequential([\n",
    "        # Layers\n",
    "        tf.keras.layers.Input(shape=(1,)),\n",
    "        tf.keras.layers.Dense(1), # Layer for regression\n",
    "    ])\n",
    "\n",
    "    model.compile(\n",
    "        optimizer=config['optimizer'],\n",
    "        loss=config['loss_func'],\n",
    "        run_eagerly=config['eager_mode'],\n",
    "    )\n",
    "\n",
    "    with tf.compat.v1.Session() as sess:\n",
    "        r = model.fit(X, Y, validation_data=(X,Y), epochs=config['epochs'], batch_size=config['batch_size'])\n",
    "        wandb.tensorflow.log(tf.compat.v1.summary.merge_all())\n",
    "        wandb.log({'loss': r.history['loss'][-1], 'log_exponential_growth_factor': model.layers[0].get_weights()[0][0,0]})\n",
    "\n",
    "        wandb_data = [[x,y] for (x,y) in zip(np.arange(0, config['epochs'], 1), r.history['loss'])]\n",
    "        table = wandb.Table(data=wandb_data, columns = [\"epoch\", \"loss\"])\n",
    "        wandb.log({\"loss_against_epochs\" : wandb.plot.line(table, \"epoch\", \"loss\", title=\"Training loss\")})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb.agent(sweep_id, sweep_func)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb.finish()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "4ae95049be29ccc4e9e63d75792906e6275d896119b1dc25df5d9e2259f80455"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
